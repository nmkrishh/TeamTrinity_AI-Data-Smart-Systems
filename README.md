# TeamTrinity_AI-Data-Smart-Systems

## ğŸ‘¥ Team Name
**Team Trinity**

## ğŸš€ Project Name
**Trinity AI â€“ Low Bandwidth Chatbot**

## ğŸ§  Track
AI, Data & Smart Systems

---

## ğŸ“Œ Problem Statement

### Low-Bandwidth AI Assistant for Rural Areas

Many rural areas face poor internet connectivity and limited access to advanced AI platforms. Modern AI systems require high-speed internet and powerful infrastructure, making them inaccessible for students and teachers in low-resource regions.

This creates a digital divide where rural learners cannot benefit from AI-powered education tools.

Trinity AI solves this by providing a lightweight AI assistant that runs locally using a small language model, requiring minimal internet bandwidth.

---

## ğŸ’¡ Solution Overview

Trinity AI is an offline-capable AI assistant designed for rural environments.

Instead of relying on cloud APIs, it uses a locally hosted language model via Ollama, reducing bandwidth usage and enabling deployment in low-connectivity areas.

It also includes:
- SQLite-based local caching
- Greeting detection for fast responses
- Optimized short-answer responses
- Low-resource backend architecture

---

## ğŸ—ï¸ Architecture

User â†’ Frontend (HTML/CSS/JS) â†’ Flask Backend â†’ Ollama Local LLM â†’ SQLite Cache

The model runs locally to ensure minimal internet dependency.

---

## ğŸ› ï¸ Tech Stack

Frontend:
- HTML
- CSS
- JavaScript

Backend:
- Python (Flask)
- SQLite (Local Cache)

AI Model:
- Ollama
- qwen2.5-coder:3b (locally hosted)

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Install Ollama
Download from:
https://ollama.com

### 2ï¸âƒ£ Pull the Model

ollama pull qwen2.5-coder:3b


### 3ï¸âƒ£ Start Ollama

ollama run qwen2.5-coder:3b


(Then press Ctrl+C to stop after confirming it works.)


### 4ï¸âƒ£ Run Backend

python app.py


### 5ï¸âƒ£ Run Frontend
Open `index.html` using Live Server or your preferred method.

---

## ğŸ“¶ Why Low Bandwidth?

- No cloud API calls required
- Model runs locally
- Responses are short and optimized
- Cache reduces repeated computation
- Designed for rural offline environments

---

## ğŸ‘¨â€ğŸ’» Team Members

Krishna Verma â€“ Frontend & AI Integration  
Devansh Gupta â€“ Frontend & UI/UX  
Divyanshu Sharma â€“ Backend Developer  

---

## ğŸ“Œ Note

This project requires Ollama and the qwen2.5-coder:3b model to be installed locally.

The AI engine runs fully offline to support rural low-bandwidth deployment scenarios.

---
